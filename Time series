import numpy as np
import pandas as pd
import yfinance as yf
import gym
from gym import spaces
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# =========================
# 1. DATA LOADING
# =========================

def load_data(symbol="^GSPC", start="2010-01-01", end="2025-01-01"):
    df = yf.download(symbol, start=start, end=end)
    df = df[['Open', 'High', 'Low', 'Close', 'Volume']]
    df.dropna(inplace=True)

    df['Return'] = df['Close'].pct_change()
    df['MA10'] = df['Close'].rolling(10).mean()
    df['MA50'] = df['Close'].rolling(50).mean()
    df['Volatility'] = df['Return'].rolling(10).std()

    df.dropna(inplace=True)
    return df

# =========================
# 2. TRADING ENVIRONMENT
# =========================

class TradingEnv(gym.Env):
    def __init__(self, df, initial_balance=10000):
        super(TradingEnv, self).__init__()

        self.df = df.reset_index(drop=True)
        self.initial_balance = initial_balance

        self.action_space = spaces.Discrete(3)  # 0 = Hold, 1 = Buy, 2 = Sell
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(self.df.shape[1],), dtype=np.float32
        )

        self.reset()

    def reset(self):
        self.current_step = 0
        self.balance = self.initial_balance
        self.shares_held = 0
        self.net_worth = self.initial_balance
        self.max_net_worth = self.initial_balance
        self.trades = []

        return self._next_observation()

    def _next_observation(self):
        obs = self.df.iloc[self.current_step].values
        return obs.astype(np.float32)

    def step(self, action):
        current_price = self.df.iloc[self.current_step]['Close']

        reward = 0

        if action == 1:  # Buy
            if self.balance > 0:
                self.shares_held = self.balance / current_price
                self.balance = 0
                self.trades.append(("buy", self.current_step, current_price))

        elif action == 2:  # Sell
            if self.shares_held > 0:
                self.balance = self.shares_held * current_price
                self.shares_held = 0
                self.trades.append(("sell", self.current_step, current_price))

        self.current_step += 1

        done = self.current_step >= len(self.df) - 1

        self.net_worth = self.balance + self.shares_held * current_price
        self.max_net_worth = max(self.max_net_worth, self.net_worth)

        # Reward: change in net worth
        reward = self.net_worth - self.initial_balance

        obs = self._next_observation() if not done else np.zeros(self.df.shape[1])

        return obs, reward, done, {}

# =========================
# 3. DQN NETWORK
# =========================

class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, output_dim)
        )

    def forward(self, x):
        return self.net(x)

# =========================
# 4. DQN AGENT
# =========================

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size

        self.memory = deque(maxlen=10000)
        self.gamma = 0.99
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.lr = 0.001

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.model = DQN(state_size, action_size).to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        self.loss_fn = nn.MSELoss()

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)

        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        with torch.no_grad():
            q_values = self.model(state)
        return torch.argmax(q_values).item()

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def replay(self, batch_size=32):
        if len(self.memory) < batch_size:
            return

        batch = random.sample(self.memory, batch_size)

        for state, action, reward, next_state, done in batch:
            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(self.device)

            target = reward
            if not done:
                target = reward + self.gamma * torch.max(self.model(next_state)).item()

            target_f = self.model(state)
            target_f[0][action] = target

            loss = self.loss_fn(self.model(state), target_f.detach())
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# =========================
# 5. TRAINING
# =========================

def train_agent(env, agent, episodes=20):
    for e in range(episodes):
        state = env.reset()
        total_reward = 0

        for time in range(len(env.df) - 1):
            action = agent.act(state)
            next_state, reward, done, _ = env.step(action)
            agent.remember(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward

            agent.replay(32)

            if done:
                print(f"Episode {e+1}/{episodes}, Net Worth: {env.net_worth:.2f}, Epsilon: {agent.epsilon:.4f}")
                break

    torch.save(agent.model.state_dict(), "dqn_trading_model.pth")
    print("Model saved as dqn_trading_model.pth")

# =========================
# 6. BACKTESTING & METRICS
# =========================

def calculate_metrics(net_worths):
    returns = pd.Series(net_worths).pct_change().dropna()

    cumulative_return = (net_worths[-1] / net_worths[0]) - 1
    sharpe_ratio = np.sqrt(252) * returns.mean() / returns.std()

    cumulative = pd.Series(net_worths)
    drawdown = (cumulative.cummax() - cumulative) / cumulative.cummax()
    max_drawdown = drawdown.max()

    return cumulative_return, sharpe_ratio, max_drawdown

def backtest(env, agent):
    state = env.reset()
    net_worths = []

    for _ in range(len(env.df) - 1):
        action = agent.act(state)
        next_state, reward, done, _ = env.step(action)
        state = next_state
        net_worths.append(env.net_worth)
        if done:
            break

    return net_worths

def buy_and_hold(df, initial_balance=10000):
    start_price = df.iloc[0]['Close']
    shares = initial_balance / start_price
    net_worths = shares * df['Close'].values
    return net_worths

# =========================
# 7. MAIN
# =========================

def main():
    df = load_data()

    split = int(len(df) * 0.8)
    train_df = df.iloc[:split]
    test_df = df.iloc[split:]

    scaler = StandardScaler()
    train_df_scaled = pd.DataFrame(scaler.fit_transform(train_df), columns=train_df.columns)
    test_df_scaled = pd.DataFrame(scaler.transform(test_df), columns=test_df.columns)

    train_env = TradingEnv(train_df_scaled)
    test_env = TradingEnv(test_df_scaled)

    state_size = train_env.observation_space.shape[0]
    action_size = train_env.action_space.n

    agent = DQNAgent(state_size, action_size)

    print("Training started...")
    train_agent(train_env, agent, episodes=20)

    print("Backtesting on test data...")
    dqn_net_worths = backtest(test_env, agent)
    bh_net_worths = buy_and_hold(test_df)

    dqn_metrics = calculate_metrics(dqn_net_worths)
    bh_metrics = calculate_metrics(bh_net_worths)

    print("\n===== PERFORMANCE COMPARISON =====")
    print("Strategy       | Cumulative Return | Sharpe Ratio | Max Drawdown")
    print("DQN Agent      | {:.2f}             | {:.2f}        | {:.2f}".format(*dqn_metrics))
    print("Buy and Hold   | {:.2f}             | {:.2f}        | {:.2f}".format(*bh_metrics))

    plt.plot(dqn_net_worths, label="DQN Agent")
    plt.plot(bh_net_worths[:len(dqn_net_worths)], label="Buy & Hold")
    plt.legend()
    plt.title("Equity Curve Comparison")
    plt.show()

if __name__ == "__main__":
    main()
